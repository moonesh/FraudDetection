package com.datamantra.spark.pipeline

import org.apache.log4j.Logger
import org.apache.spark.ml.PipelineStage
import org.apache.spark.ml.classification.DecisionTreeClassifier
import org.apache.spark.ml.clustering.KMeans
import org.apache.spark.ml.feature.{VectorAssembler, OneHotEncoder, StringIndexer}
import org.apache.spark.ml.regression.DecisionTreeRegressor
import org.apache.spark.sql.types.{NumericType, StringType, StructType}

import scala.collection.mutable

/**
 * Created by kafka on 9/5/18.
 */
object BuildPipeline {

  val logger = Logger.getLogger(getClass.getName)
  
  // StrinIndexer - 1st Stage of Pipeline 
  //This will transform all the selected columns into Double Values because the ML Algo will not understand String Values but Only Doubble Values
  

  def createStringIndexer(columns:List[String]) = {
    columns.map(column => {
          val stringIndexer = new StringIndexer()
          stringIndexer.setInputCol(column).setOutputCol(s"${column}_indexed")
          stringIndexer
    })
  }

  
  // OneHotEncoder - 2nd Stage of Pipeline 
  //This will normalize the Double Values (generated by StringIndexer) 
  
  
  def createOneHotEncoder(columns:List[String]) = {

    columns.map(column => {
      val oneHotEncoder = new OneHotEncoder()
      oneHotEncoder.setInputCol(s"${column}_indexed").setOutputCol(s"${column}_encoded")
      oneHotEncoder
    })
  }

  
  // VectorAssembler - 3rd Stage of Pipeline 
 //This will assemble all the transformed columns into ONE Coloumn - this column is called FEATURE COLUMN and the Value of this column is an Vector.
 //This FEATURE COLUMN will enable us to Train ON this Data Frame and  will be served as an input to the Model Creation Algorithm.
  
  // But Training on this Data Frame will not be accurate as the Data is NOT Balanced. Which means The number of Fraud records are too less than Non-fraund.
  // This is where K-MEANS Algo comes into play. 
  
  
  // How would K-Means Work here?   This will basically reduce the Normal (Non-Fraud) Transactions as explained below (Check the Diagram):
  //1. The output of Vector Assembler i.e. FETAURE COLUMN is split  into 2 Data Frames - NON-FRaud and FRaud.
  //2. The Non-Fraud DF will be the first input to K-MEANS (which basically needs to be reduced)
  //3. Second Input to KMEANS is No. of CLUSTERED CENTROID = No of Fraud Transactions
  //4. Now K-MEANS will group all the non-fraud transactions into these centroids. As a result the no. of non-fraud transactions are reduced to No. of FRaud Transactions.
       
  //5. Now COmbine the FraudTransactions DF with the above Non-Fraud transformed DF and this becomes a Balanced DF.
  //6. Now apply Random Forest Algo on Balaced DF. The Random Forest will train on this data and create a Model and saved to File System.
  
  
  
  
  def createVectorAssembler(featureColumns:List[String]) = {
    val vectorAssembler = new VectorAssembler()
    vectorAssembler.setInputCols(featureColumns.toArray).setOutputCol("features")
  }


  def createFeaturePipeline(schema:StructType, columns:List[String]):Array[PipelineStage] = {
    val featureColumns = mutable.ArrayBuffer[String]()
    val preprocessingStages = schema.fields.filter(field => columns.contains(field.name)).flatMap(field => {

      field.dataType match {
        case s: StringType => {

          val stringIndexer = new StringIndexer()
          stringIndexer.setInputCol(field.name).setOutputCol(s"${field.name}_indexed")

          val oneHotEncoder = new OneHotEncoder()
          oneHotEncoder.setInputCol(s"${field.name}_indexed").setOutputCol(s"${field.name}_encoded")

          featureColumns += (s"${field.name}_encoded")
          Array[PipelineStage](stringIndexer, oneHotEncoder)
          //Array[PipelineStage](stringIndexer)
        }

        case n: NumericType => {
          featureColumns += (field.name)
          Array.empty[PipelineStage]
        }
        case _ => {
          Array.empty[PipelineStage]
        }
      }

    })

    val vectorAssembler = new VectorAssembler()
    vectorAssembler.setInputCols(featureColumns.toArray).setOutputCol("features")

    (preprocessingStages :+ vectorAssembler)

  }


  def createStringIndexerPipeline(schema:StructType, columns:List[String]):Array[PipelineStage] = {
    val featureColumns = mutable.ArrayBuffer[String]()
    val preprocessingStages = schema.fields.filter(field => columns.contains(field.name)).flatMap(field => {

      field.dataType match {
        case s: StringType => {

          val stringIndexer = new StringIndexer()
          stringIndexer.setInputCol(field.name).setOutputCol(s"${field.name}_indexed")

          featureColumns += (s"${field.name}_indexed")
          Array[PipelineStage](stringIndexer)
        }

        case n: NumericType => {
          featureColumns += (field.name)
          Array.empty[PipelineStage]
        }
        case _ => {
          Array.empty[PipelineStage]
        }
      }

    })


    val vectorAssembler = new VectorAssembler()
    vectorAssembler.setInputCols(featureColumns.toArray).setOutputCol("features")

    (preprocessingStages :+ vectorAssembler)

  }
}
